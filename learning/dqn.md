### NCHW形式
畳み込み層には、NCHW形式のテンソルで局面を入力している。つまり、N: バッチ、C: チャネル、H: 高さ、W: 幅、の4種である。
石のある位置を1、それ以外を0としてる

### 並列Gym環境
強化学習では大量の試行を行う必要があり、シミュレータが高速であることが重要。そのため、環境を複数並列で実行できるインタフェースを用意している。
import gym

## 強化学習入門
### 概要
教師データを用いないで、エージェントが自分自身で最適な行動を発見する学習方法を、強化学習という。
エージェントは環境との相互作用を通して得られる報酬を手がかりに学習を行う。
強化学習では通常、選択できる行動、得られる報酬、次の状態への遷移は、現在の状態のみに依存すると仮定している。このような枠組みのことをマルコフ決定過程と呼ぶ。一般的には次の状態への遷移は確率的に発生するが、リバーシのようなボードゲームでは、遷移は決定論的になる。

#### エピソード
エージェントが行動を開始してから、一連の行動を終了するまでの単位をエピソードと呼ぶ。

#### 経験
エージェントは環境との相互作用で得られて情報を記録して、そのデータを基に学習を行う。記録する情報は｛現在の状態、行動、次の状態、報酬｝の4の組。この4つの組のことを経験と呼ぶ。

#### 行動
a

#### 報酬
報酬は慣例的に記号$r$で表す。

#### 収益
ある時間ステップでの報酬を見積もった値を収益という。収益は単純な場合は、報酬の合計となるが、将来のステップでの報酬を割り引いて見積もる場合がある。

#### 方策
エージェントが行動を選択する判断基準を方策と呼ぶ。
エージェントが常にその時点でのエージェント基準で最善の行動を選択した場合、新しい行動を発見する機会がなくなる。そのため、少しは新しい行動を試すことがひつようとなる。これには、$\epsilon$グリーディー法やソフトマックス行動選択規則といった方法がある。
方策は慣例的に記号$\pi$で表す。

#### 状態価値
ある状態から将来的に得られる報酬の期待値をその状態の状態価値と呼ぶ。状態価値は慣例的にVで表され、状態価値が関数の場合、ステップtの状態を$s_t$とすると、状態価値は$V(s_t)$で表す。

## Q学習
何ステップか先の価値と現在の価値の差を学習する方法をTD学習と呼ぶ。Q学習はいくつかあるTD学習の一つ。
Q学習では、あるステップ$t$の1ステップ後の状態$s_{t+1}$の行動価値$Q(s_{t+1},a)$を使って、現在の状態$s_{t}$の行動価値を更新する！
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_{t+1}+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$$ここで、$\alpha$は学習率、$r_{t+1}$は状態$s_t$で行動$a_t$をとった際に得られる即時報酬、$\gamma$は割引率を表す。
$\max_aQ(s_{t+1},a)$は、1ステップ後の状態$s_{t+1}$で行動価値が最大となる行動をとった場合の行動価値を意味している。つまり、Q学習では、1ステップ後の価値を最大の行動価値で見積もって、現在の行動価値との差を学習している！

### $\epsilon$グリーディー方策
基本的にその時点の行動価値関数で価値が最大となる行動を取るが、一定の小さな確率$\epsilon$でランダムな行動を取る。$\epsilon$ははじめは大きく、学習が進むにつれて小くしていくと良い。

## DQN
ここではAtariのビデオゲームの学習を例に取る。
Q学習の式、
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_{t+1}+\gamma\max_aQ(s_{t+1},a)-Q(s_t,a_t)]$$において、$Q(s_t,a_t)$が、ニューラルネットワークによって表現されている。状態$s_t$が入力する画像。$a_t$がコントローラの各入力に対応する。
関数$Q$には$a_t$が引数になっているのに、ニューラルネットワークには$a_t$が入力されていないが、これはニューラルネットワークではすべての行動の行動価値を同時に出力しているため、1つ1つの行動を入力する必要がないことによる。

### 経験再生
とは

### 方策ネットワークとターゲットネットワークのパラメータ
$\max_aQ(s_{t+1},a_t,\theta)$の部分のパラメータ$\theta$にのみ、$\theta_i$の数ステップ前のパラメータ、$\theta_i^{-}$を用いる。
**なぜ？？？**

### 損失関数
Qが線形関数の場合は、上記の式を使用してパラメータの更新が行えるが、ニューラルネットワークのパラメータの更新では、損失関数を計算して確率的勾配降下法で荒メータの更新を行う。行動価値の期待値$r_{t+1}+\gamma{\max_aQ(s_{t+1},a;\theta_i^{-})}$とニューラルネットワークの予想$Q(s_t,a_t;\theta_i)$の誤差を、損失関数$L(\theta_i)$で見積る。損失関数二乗誤差を用いた場合、損失関数は以下の式になる
$$L(\theta_i)=\left[\left(r_{t+1}+\gamma{\max_aQ(s_{t+1},a;\theta_i^{-})}-Q(s_t,a_t;\theta_i)\right)^2\right]$$DQNでは、二乗誤差をそのまま用いずに、誤差が-1から1の間は二乗誤差を使用し、それ以外の範囲では、差の絶対値を使用する。この損失関数は、**Huberloss**と呼ばれている。

## PyTorch
[公式](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)に詳しいチュートリアルがある。
GPUで計算する処理を、for文やif文などで記述すると、処理が一桁ほど低下してしまう。

## リバーシのDQN実装
自己対局を行う、ミニマックスの考慮、更新間隔

### リバーシのニューラルネットワーク
チュートリアルでは、各ステップのRGB画像を入力として、畳み込みニューラルネットワークで処理して、左加速もしくは右加速の行動価値を出力していた。リバーシでは以下のような違いがある。

#### 入力特徴量
リバーシでは、バンメンの石の配置を8*8の2値画像として表現する
石には、黒と白があるので、それぞれにチャンネルを割り当てて、2チャンネルの画像とする。ここでは、先手と後手の区別をしないで、同じニューラルネットワークで扱えるように、現在手番を持っている側のチャンネルと相手側のチャンネルの2チャンネルとする。

#### 出力表現
各手の行動価値を出力する。ニューラルネットワークでは、合法手を判定できないため、非合法手も含めて、各座標に対応する手の行動価値を出力する。
0-63のインデックスを割り当てている

#### ニューラルネットワークの構成
- 10層の畳み込みニューラルネットワーク
- 各畳み込み層には、カーネルサイズ3*3、フィルター数192
- 畳み込み層の後に、256ユニットの全結合層をつなげる
- 活性化関数はすべてReLUとする。
- 出力層は、各行動（64の座標とパス）に対応するユニットの全結合層とする。
- 出力層の活性化関数は、行動価値が-1~1おなるように、tanhとする。


## 自分の環境
PyTorchをLOCALにインストールした。
```
pip install torch torchvision torchaudio
```
訓練用にcreversiも入れた
```
pip install https://github.com/TadaoYamaoka/creversi/releases/download/v0.0.0/creversi-0.0.0-cp36-cp36m-linux_x86_64.whl
```
GLIBCのバージョンが低い、と怒られたので、[ここ](https://www.saintsouth.net/blog/update-libstdcpp-on-centos6/)をみながらgcc7.5.0をインストールして解決？
 



## 疑問
- 入力が2チャンネル画像とは？


